{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb47531-1fa8-4d32-9b71-b8e104022fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 17:44:38.121286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 17:44:38.243326: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-01 17:44:39.237567: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-01 17:44:39.237621: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-02-01 17:44:39.237627: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/gridsan/fbirnbaum/.local/lib/python3.9/site-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
      "  warnings.warn(\n",
      "/state/partition1/slurm_tmp/24924496.0.0/ipykernel_2306677/1956004192.py:22: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import src.models_and_optimizers as model_utils\n",
    "import yaml\n",
    "from types import SimpleNamespace\n",
    "from clip_main import get_wds_loaders\n",
    "from transformers import EsmTokenizer\n",
    "import src.data_utils as data_utils\n",
    "import torch\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from tmtools.io import get_structure, get_residue_data\n",
    "from tmtools import tm_align\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from torch.cuda.amp import autocast\n",
    "import tmscoring\n",
    "import json\n",
    "import copy\n",
    "from scipy.stats.stats import pearsonr \n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import glob\n",
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c422249-f113-44d2-b783-b93842f270fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mask peptide sequence information\n",
    "def mask_peptide(seq_batch, coords_batch, pdb):\n",
    "    chain_len_dicts = {}\n",
    "    lens = coords_batch['seq_lens']\n",
    "    chain_len_dicts['protein'] = max(lens)\n",
    "    chain_len_dicts['peptide'] = min(lens)\n",
    "    from_back = lens[1] == min(lens)\n",
    "    if from_back:\n",
    "        seq_batch['string_sequence'][0] = seq_batch['string_sequence'][0][:-peptide_len] + 'X'*peptide_len\n",
    "        seq_batch['seq_loss_mask'][0][:,:-1*peptide_len] = False\n",
    "        seq_batch['seq_loss_mask'][1][:,:-1*peptide_len] = False\n",
    "    else:\n",
    "        seq_batch['string_sequence'][0] = 'X'*peptide_len + seq_batch['string_sequence'][0][peptide_len:]\n",
    "        seq_batch['seq_loss_mask'][0][:,peptide_len:] = False\n",
    "        seq_batch['seq_loss_mask'][1][:,peptide_len:] = False\n",
    "    return seq_batch\n",
    "\n",
    "def mask_all(seq_batch, pdb):\n",
    "    seq_batch['string_sequence'][0] = 'X'*len(seq_batch['string_sequence'][0])\n",
    "    seq_batch['seq_loss_mask'][0][:,:] = False\n",
    "    seq_batch['seq_loss_mask'][1][:,:] = False\n",
    "    return seq_batch\n",
    "\n",
    "## Extract kNN info\n",
    "def extract_knn(X, eps, top_k):\n",
    "    # Convolutional network on NCHW\n",
    "    dX = torch.unsqueeze(X, 1) - torch.unsqueeze(X, 2)\n",
    "    D = torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
    "    mask_2D = torch.ones(D.shape)\n",
    "    D *= mask_2D\n",
    "\n",
    "    # Identify k nearest neighbors (including self)\n",
    "    D_max, _ = torch.max(D, -1, keepdim=True)\n",
    "    D_adjust = D + (1. - mask_2D) * D_max\n",
    "    D_neighbors, E_idx = torch.topk(D_adjust, top_k, dim=-1, largest=False)\n",
    "    return D_neighbors, E_idx\n",
    "\n",
    "## Find residues at the interface between 2 chains\n",
    "def get_interaction_res(coords_batch, pdb, top_k, threshold=5, remove_far=False, mres=None, dist_threshold=None):\n",
    "    chain_len_dicts = {}\n",
    "    lens = coords_batch['seq_lens'][0]\n",
    "    chain_len_dicts['protein'] = max(lens)\n",
    "    chain_len_dicts['peptide'] = min(lens)\n",
    "    if len(lens) > 1:\n",
    "        from_back = lens[1] == min(lens)\n",
    "    else:\n",
    "        from_back = True\n",
    "    top_k = min(top_k, coords_batch['coords'][0].size(1))\n",
    "    D_neighbors, E_idx = extract_knn(coords_batch['coords'][0][:,:,1,:], eps=1e-6, top_k=top_k) \n",
    "    if from_back:\n",
    "        interaction_res = set(range(chain_len_dicts['protein'], chain_len_dicts['protein']+chain_len_dicts['peptide']))\n",
    "    else:\n",
    "        interaction_res = set(range(0, chain_len_dicts['peptide']))\n",
    "    # interaction_res = set(range(0, chain_len_dicts['protein']+chain_len_dicts['peptide']))\n",
    "    if mres is not None:\n",
    "        interaction_res = set(mres)\n",
    "    if top_k == 0:\n",
    "        return list(interaction_res)\n",
    "    prot_to_add = set()\n",
    "    for res in interaction_res:\n",
    "        neighs_to_add = list()\n",
    "        for dist, neigh in zip(D_neighbors[0, res], E_idx[0, res]):\n",
    "            if dist_threshold and dist > dist_threshold:\n",
    "                continue\n",
    "            neighs_to_add.append(neigh)\n",
    "        prot_to_add = prot_to_add.union(set(neighs_to_add))\n",
    "    interaction_res = list(interaction_res.union(prot_to_add))\n",
    "    if remove_far:\n",
    "        to_remove = []\n",
    "        chain_lens = torch.cat([torch.zeros(lens[0]), torch.ones(lens[1])])\n",
    "        for res in interaction_res:\n",
    "            nother = 0\n",
    "            \n",
    "            opp = 1 - chain_lens[res].item()\n",
    "            for nres in E_idx[0, res]:\n",
    "                if chain_lens[nres].item() == opp:\n",
    "                    nother += 1\n",
    "\n",
    "            if nother < threshold:\n",
    "                to_remove.append(res)\n",
    "        for res in to_remove:\n",
    "            interaction_res.remove(res)\n",
    "    return interaction_res\n",
    "\n",
    "## Get distances between residues at the interface between 2 chains\n",
    "def get_inter_dists(coords_batch, interaction_res, eps=1e-6):\n",
    "    chain_len_dicts = {}\n",
    "    chains, lens = torch.unique_consecutive(coords_batch['chain_lens'][0][0], return_counts=True)\n",
    "    chain_len_dicts['protein'] = torch.max(lens)\n",
    "    chain_len_dicts['peptide'] = torch.min(lens)\n",
    "    from_back = torch.argmin(lens) == 1\n",
    "    if from_back:\n",
    "        pep_res = list(range(chain_len_dicts['protein'], chain_len_dicts['protein']+chain_len_dicts['peptide']))\n",
    "        prot_res = ppe_res = list(range(0, chain_len_dicts['protein']))\n",
    "    else:\n",
    "        pep_res = list(range(0, chain_len_dicts['peptide']))\n",
    "        prot_res = list(range(chain_len_dicts['peptide'], chain_len_dicts['peptide']+chain_len_dicts['protein']))\n",
    "    pep_coords = coords_batch['coords'][0][:,pep_res,1,:]\n",
    "    prot_coords = coords_batch['coords'][0][:,prot_res,1,:]\n",
    "    inter_dists = []\n",
    "    for res in range(chain_len_dicts['protein'] + chain_len_dicts['peptide']):\n",
    "        if not res in interaction_res:\n",
    "            inter_dists.append(np.nan)\n",
    "            continue\n",
    "        res_coord = coords_batch['coords'][0][:,res,1,:]\n",
    "        if res in pep_res:\n",
    "            dists = torch.sum((res_coord - prot_coords)**2, dim=2)\n",
    "        else:\n",
    "            dists = torch.sum((res_coord - pep_coords)**2, dim=2)\n",
    "        inter_dists.append(torch.min(dists))\n",
    "    inter_dists = np.array(inter_dists)\n",
    "    inter_dists = inter_dists / np.nanmax(inter_dists)\n",
    "    inter_dists = np.nan_to_num(inter_dists)\n",
    "    return inter_dists\n",
    "            \n",
    "## Mask the structure of the peptide\n",
    "def mask_peptide_struct(coord_data, coords_batch, pdb):\n",
    "    peptide_res = get_interaction_res(coords_batch, pdb, top_k=0)\n",
    "    protein_mask = torch.ones(coord_data['x_mask'].shape).to(dtype=coord_data['x_mask'].dtype, device=coord_data['x_mask'].device)\n",
    "    protein_mask[:,peptide_res] = 0\n",
    "    coord_data['x_mask'] *= protein_mask\n",
    "    coord_data['X'] *= protein_mask.unsqueeze(-1).unsqueeze(-1)\n",
    "    return coord_data\n",
    "\n",
    "## Get sequence and structure embeddings from RLA \n",
    "def get_seq_and_struct_features(model, tokenizer, batch, pdb=None, seq_mask=None, struct_mask=None, focus=None, top_k=30, remove_far=False, mres=None,\n",
    "                                prot_len=None, add_ends=False, ends_k=0, threshold=1, seq_only=False, dist_threshold=None):\n",
    "    seq_batch, coords_batch = batch\n",
    "    if prot_len is not None:\n",
    "        seq_batch['string_sequence'][0] = [seq_batch['string_sequence'][0][:prot_len] + 25*'G' + seq_batch['string_sequence'][0][prot_len:]]\n",
    "        base_len = len(seq_batch['string_sequence'][0])\n",
    "        pos_embs = torch.arange(base_len+25).unsqueeze(0)\n",
    "        seq_mask = torch.ones(base_len+25).to(dtype=torch.bool).unsqueeze(0)\n",
    "        seq_batch['pos_embs'] = [pos_embs, seq_mask]\n",
    "        seq_batch['placeholder_mask'] = [seq_mask, seq_mask]\n",
    "        seq_batch['seq_loss_mask'] = [seq_mask, seq_mask]\n",
    "    if add_ends:\n",
    "        base_len = len(seq_batch['string_sequence'][0])\n",
    "        seq_batch['string_sequence'][0] = ends_k*'X' + seq_batch['string_sequence'][0] + ends_k*'X'\n",
    "        pos_embs = torch.arange(base_len+2*ends_k).unsqueeze(0)\n",
    "        seq_mask = torch.zeros(base_len+2*ends_k)\n",
    "        seq_mask[ends_k:-ends_k] = 1\n",
    "        seq_mask = seq_mask.to(dtype=torch.bool).unsqueeze(0)\n",
    "        seq_batch['pos_embs'] = [pos_embs, seq_mask]\n",
    "        seq_batch['placeholder_mask'] = [seq_mask, seq_mask]\n",
    "        seq_batch['seq_loss_mask'] = [seq_mask, seq_mask]\n",
    "    if seq_mask == 'peptide':\n",
    "        seq_batch = mask_peptide(seq_batch, coords_batch, pdb)\n",
    "    if seq_mask == 'all':\n",
    "        seq_batch = mask_all(seq_batch, pdb)\n",
    "    seqs = seq_batch['string_sequence']\n",
    "    text_inp = tokenizer(seqs, return_tensors='pt', padding=True, truncation=True, max_length=1024+2)\n",
    "    text_inp['position_ids'] = seq_batch['pos_embs'][0]\n",
    "    text_inp = {k: v.to('cuda') for k, v in text_inp.items()}\n",
    "    if not seq_only:\n",
    "        coord_data = data_utils.construct_gnn_inp(coords_batch, device='cuda', half_precision=True)\n",
    "        if struct_mask=='peptide':\n",
    "            coord_data = mask_peptide_struct(coord_data, coords_batch, pdb)\n",
    "        gnn_features, text_features, logit_scale = model(text_inp, coord_data)\n",
    "    else:\n",
    "        gnn_features, text_features, logit_scale = model(text_inp, None)\n",
    "    new_text_features, _, new_text_mask = data_utils.postprocess_text_features(\n",
    "        text_features=text_features, \n",
    "        inp_dict=text_inp, \n",
    "        tokenizer=tokenizer, \n",
    "        placeholder_mask=seq_batch['placeholder_mask'][0])\n",
    "    if prot_len is not None:\n",
    "        new_text_features = torch.cat((new_text_features[:,:prot_len,:], new_text_features[:,prot_len+25:,:]), dim=1)\n",
    "        new_text_mask = torch.cat((new_text_mask[:,:prot_len], new_text_mask[:,prot_len+25:]), dim=1)\n",
    "    if focus:\n",
    "        focus_res = get_interaction_res(coords_batch, pdb, top_k, remove_far = remove_far, threshold = threshold, mres = mres, dist_threshold = dist_threshold)\n",
    "        focus_mask = torch.zeros(coords_batch['coords'][1].shape).to(dtype=coords_batch['coords'][1].dtype, device=new_text_features.device)\n",
    "        focus_mask[:,focus_res] = True\n",
    "    else:\n",
    "        focus_mask = torch.zeros(coords_batch['coords'][1].shape).to(dtype=coords_batch['coords'][1].dtype, device=new_text_features.device)\n",
    "    return {\n",
    "        'text': new_text_features, # text feature\n",
    "        'gnn': gnn_features, # gnn feature\n",
    "        'seq_mask_with_burn_in': seq_batch['seq_loss_mask'][0], # sequence mask of what's supervised\n",
    "        'coord_mask_with_burn_in': coords_batch['coords_loss_mask'][0], # coord mask of what's supervised\n",
    "        'seq_mask_no_burn_in': new_text_mask.bool(), # sequence mask of what's valid (e.g., not padded)\n",
    "        'coord_mask_no_burn_in': coords_batch['coords'][1], # coord mask of what's valid\n",
    "        'focus_mask': focus_mask, # focus mask of what residues to use to calculate score\n",
    "    }\n",
    "\n",
    "## Calculate RLA score\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "def calc_sim(all_outputs):\n",
    "    all_sims = []\n",
    "    all_sims_burn = []\n",
    "    for output in all_outputs:\n",
    "        t = output['text'][output['seq_mask_no_burn_in']]\n",
    "        g = output['gnn'][output['coord_mask_no_burn_in']]\n",
    "        sim = (t.unsqueeze(1) @ g.unsqueeze(-1)).squeeze(1).squeeze(1)\n",
    "        all_sims.append(torch.mean(sim))\n",
    "\n",
    "    return all_sims\n",
    "\n",
    "# Get pdb ids of proteins in a batch\n",
    "def get_decoy_ids(wds_path):\n",
    "    cols = ['inp.pyd']\n",
    "    wd_ds = wds.WebDataset(wds_path).decode().to_tuple(*cols)\n",
    "    batched_ds = wd_ds.batched(1, collation_fn=loaders_utils.custom_collation_fn)\n",
    "    decoy_ids = []\n",
    "    for i, b in enumerate(wd_ds):\n",
    "        decoy_ids.append(b[0]['pdb_id'][0])\n",
    "    return decoy_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7aefe-2d89-4a54-b817-145bffc14c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESM mutation analysis functions\n",
    "def get_muts(wt, mut):\n",
    "    inds = []\n",
    "    muts = []\n",
    "    for i, (wchar, mchar) in enumerate(zip(wt, mut)):\n",
    "        if wchar != mchar:\n",
    "            inds.append(i)\n",
    "            muts.append(mchar)\n",
    "    return inds, muts\n",
    "\n",
    "def score_mut(wt, idx, mt, token_probs, alphabet):\n",
    "    wt = wt[idx]\n",
    "    wt_encoded, mt_encoded = alphabet.get_idx(wt), alphabet.get_idx(mt)\n",
    "\n",
    "    # add 1 for BOS\n",
    "    score = token_probs[0, 1 + idx, mt_encoded] - token_probs[0, 1 + idx, wt_encoded]\n",
    "    return score.item()\n",
    "\n",
    "def score_protein(idxs, mts, wt_seq, model, alphabet):\n",
    "\n",
    "    # inference for each model\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    data = [\n",
    "        (\"protein1\", wt_seq),\n",
    "    ]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "    all_token_probs = []\n",
    "    for i in tqdm(range(batch_tokens.size(1))):\n",
    "        batch_tokens_masked = batch_tokens.clone()\n",
    "        batch_tokens_masked[0, i] = alphabet.mask_idx\n",
    "        with torch.no_grad():\n",
    "            token_probs = torch.log_softmax(\n",
    "                model(batch_tokens_masked.cuda())[\"logits\"], dim=-1\n",
    "            )\n",
    "        all_token_probs.append(token_probs[:, i])  # vocab size\n",
    "    token_probs = torch.cat(all_token_probs, dim=0).unsqueeze(0)\n",
    "    esm_predictions = []\n",
    "    for idx_list, mut_list in zip(idxs, mts):\n",
    "        mut_score = 0\n",
    "        for idx, mut in zip(idx_list, mut_list):\n",
    "            mut_score += score_mut(wt_seq, idx, mut, token_probs, alphabet)\n",
    "        if len(idx_list) == 0:\n",
    "            mut_score = np.nan\n",
    "        esm_predictions.append(mut_score)\n",
    "    return esm_predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rla-public]",
   "language": "python",
   "name": "conda-env-.conda-rla-public-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
