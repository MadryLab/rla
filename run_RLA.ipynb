{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c7284-add5-4077-95d6-57bc556175d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.models_and_optimizers as model_utils\n",
    "from types import SimpleNamespace\n",
    "from clip_main import get_wds_loaders\n",
    "from transformers import EsmTokenizer\n",
    "import src.data_utils as data_utils\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "import esm as esmlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68069b-1dd5-43cb-badf-537b6df382aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL SETUP (CHANGE PATHS AS NEEDED)\n",
    "ROOT = \"/PATH/TO/MODEL/WEIGHTS\"\n",
    "model_dir = \"version_0/\" \n",
    "dev = 'cuda:0'\n",
    "CLIP_MODE = False\n",
    "root_path = os.path.join(ROOT, model_dir)\n",
    "path = os.path.join(root_path, \"checkpoints/checkpoint_best.pt\")\n",
    "data_root = \"/PATH/TO/WDS_DIR\" #\n",
    "args_path = os.path.join(ROOT, model_dir, [u for u in os.listdir(os.path.join(ROOT, model_dir)) if u.endswith('.pt')][0])\n",
    "\n",
    "backwards_compat = {\n",
    "    'masked_rate': -1,\n",
    "    'masked_mode': 'MASK',\n",
    "    'lm_only_text': 1,\n",
    "    'lm_weight': 1,\n",
    "    'resid_weight': 1,\n",
    "    'language_head': False,\n",
    "    'language_head_type': 'MLP',\n",
    "    'zip_enabled': False,\n",
    "    'num_mutations': False,\n",
    "}\n",
    "hparams = torch.load(args_path)\n",
    "args_dict = hparams['args']\n",
    "args_dict['data_root'] = data_root\n",
    "args_dict['train_wds_path'] = 'wds_'\n",
    "args_dict['val_wds_path'] = 'wds_'\n",
    "args_dict['batch_size'] = 1\n",
    "args_dict['num_workers'] = 1\n",
    "args_dict['blacklist_file'] = ''\n",
    "for k in backwards_compat.keys():\n",
    "    if k not in args_dict:\n",
    "        args_dict[k] = backwards_compat[k]\n",
    "args = SimpleNamespace(**args_dict)\n",
    "\n",
    "print(vars(args))\n",
    "\n",
    "coordinator_params = data_utils.get_coordinator_params(args.coordinator_hparams)\n",
    "coordinator_params['num_positional_embeddings'] = args.gnn_num_pos_embs\n",
    "coordinator_params['zero_out_pos_embs']= args.gnn_zero_out_pos_embs\n",
    "coordinator_params['clip_mode'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa0497-ab5f-4488-b09a-76129329cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD MODEL (NO CHANGES NEEDED)\n",
    "args_dict['arch']= 'esm_model_150'\n",
    "trained_model = model_utils.load_model(path, args_dict['arch'], dev)\n",
    "tokenizer = EsmTokenizer.from_pretrained(args_dict['arch'])\n",
    "esm_arch = 'esm_model_150'\n",
    "esm_model = EsmModel.from_pretrained(args_dict['arch']) \n",
    "esm_model = esm_model.to(dev)\n",
    "esm_model.eval()\n",
    "esm, alphabet = esmlib.pretrained.esm2_t30_150M_UR50D()\n",
    "# esm, alphabet = esmlib.pretrained.esm1v_t33_650M_UR90S_1()\n",
    "esm = esm.eval()\n",
    "if dev == 'cuda:0':\n",
    "    esm = esm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e23d79-447d-4a43-931e-6fa028a4306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD UTIL FUNCTIONS (NO CHANGES NEEDED)\n",
    "%run 2023_12_11_rla_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897b93d-e04a-4bc5-ae88-adb2d4f9fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = trained_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc166e1c-e7c0-4358-983e-6b54e50a8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE RLA SETTING CALCULATIONS (SOME CHANGES POSSIBLE)\n",
    "seq_mask='peptide' # 'peptide' to mask peptide sequence, 'protein' to mask protein sequence\n",
    "struct_mask=None # 'peptide' to mask peptide structure, 'protein' to mask protein structure\n",
    "top_k = 30 # Num neighbors, probably want to keep at 30 but can experiment\n",
    "focus = True # RLA calculation setting that limits RLA score to interface, almost certainly want to keep True\n",
    "remove_far = True # Removes residues too far from the interface, likely want to keep True\n",
    "threshold = 1 # Threshold for remove_far calculation, likely want to keep at 1\n",
    "weight_dists = False # Weights RLA score per residue by distance from interface, likely want to keep False\n",
    "pep_weight = 1 # Weight of peptide residues relative to protein residues, likely want to keep at 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84a1277-4b88-4d98-848a-f412f3a9a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATASETS TO EVALUATE (CHANGE AS NEEDED)\n",
    "design_sets = [\"whole_structs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd632200-5960-422e-9b63-d06a8962e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f70bbb-31ac-44a7-b111-ec5958e1e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]['coords'][0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757352b-0ef8-45ff-8c71-a30a70d71294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PERFORMS RLA CALCULATION (NO CHANGES NEEDED)\n",
    "if CLIP_MODE:\n",
    "    feature_getter = get_text_and_image_features_clip\n",
    "else:\n",
    "    feature_getter = get_text_and_image_features\n",
    "    \n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "nclash_dict, Fnat_dict, Fnonnat_dict, LRMS_dict, iRMSDbb_dict, irmsdsc_dict, distance_dict, theta_dict, class_dict = {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "dicts = [nclash_dict, Fnat_dict, Fnonnat_dict, LRMS_dict, iRMSDbb_dict, irmsdsc_dict, distance_dict, theta_dict, class_dict]\n",
    "nclash_data, Fnat_data, Fnonnat_data, LRMS_data, iRMSDbb_data, irmsdsc_data, distance_data, theta_data, class_data = {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "data_dicts = [nclash_data, Fnat_data, Fnonnat_data, LRMS_data, iRMSDbb_data, irmsdsc_data, distance_data, theta_data, class_data]\n",
    "args.batch_size = 1\n",
    "args.zip_enabled = False\n",
    "args.num_mutations = 0\n",
    "args.distributed = 0\n",
    "plot_scores = []\n",
    "plot_weights = []\n",
    "plot_pep_mask = []\n",
    "plot_indices = []\n",
    "plot_X = []\n",
    "plot_seq = []\n",
    "paired_res_scores = {}\n",
    "scores_stats = {'models': [], 'seqs': [], 'rla_scores': []}\n",
    "# result_types = ['nclash', 'fnat', 'fnonnat', 'lrmsd', 'irmsdbb', 'irmsdsc', 'distance', 'theta', 'classification']\n",
    "for design_set in design_sets:\n",
    "    print(f'running on {design_set}.')\n",
    "    args.train_wds_path = f\"{design_set}.wds\"\n",
    "    args.val_wds_path = f\"{design_set}.wds\"\n",
    "    train_loader, val_loader, train_len, val_len = get_wds_loaders(args, coordinator_params, gpu=None, shuffle_train=False, val_only=True, return_count=False)\n",
    "    lens = {}\n",
    "    for i, b in tqdm(enumerate(val_loader), total=val_len):\n",
    "        lens[b[0]['pdb_id'][0]] = len(b[0]['string_sequence'][0])\n",
    "    MAX_LEN = max(lens.values())\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(val_loader, total=val_len)):\n",
    "        model = batch[0]['pdb_id'][0]\n",
    "        pep_seq = batch[0]['string_sequence'][0][:batch[1]['seq_lens'][0][0]]\n",
    "        chain_lens = torch.zeros(batch[1]['coords'][0].shape[1]).to(device = batch[1]['coords'][0].device)\n",
    "        chain_lens[batch[1]['seq_lens'][0][0]:] = 1\n",
    "        chain_lens_mask = torch.ones(batch[1]['coords'][0].shape[1]).unsqueeze(0).to(dtype=torch.bool, device = batch[1]['coords'][0].device)\n",
    "        batch[1]['chain_lens'] = [chain_lens.unsqueeze(0), chain_lens_mask]\n",
    "        with torch.no_grad():\n",
    "            with autocast(dtype=torch.float16):\n",
    "                output_dict = feature_getter(trained_model, tokenizer, batch, pdb=None, weight_dists=weight_dists, seq_mask=seq_mask, focus=focus, top_k=top_k, struct_mask=struct_mask, remove_far=remove_far, threshold=threshold, dev=dev)\n",
    "                score, scores, plot_scores, plot_weights, plot_pep_mask, plot_indices, plot_X, plot_seq = compute_score(output_dict, weight_dists, MAX_LEN, plot_scores=plot_scores, plot_weights=plot_weights, plot_pep_mask=plot_pep_mask, plot_indices=plot_indices, plot_X=plot_X, plot_seq=plot_seq, is_complex=True)\n",
    "                scores_stats['models'].append(model)\n",
    "                scores_stats['seqs'].append(pep_seq)\n",
    "                paired_res_scores[model] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eacc10-5ba2-443e-8b2d-86d7db46ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE/LOAD RAW SCORES AND SEQUENCE MAPPINGS (CHANGE JSON WRITING/READING AS NEEDED)\n",
    "import json\n",
    "\n",
    "with open('/PATH/TO/OUTPUTS/seq_mapping.json', 'w') as f:\n",
    "    json.dump(scores_stats, f)\n",
    "with open('/PATH/TO/OUTPUTS/scores.json', 'w') as f:\n",
    "    json.dump(paired_res_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-rla-test]",
   "language": "python",
   "name": "conda-env-.conda-rla-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
